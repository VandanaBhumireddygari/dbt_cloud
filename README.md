Got you â€” here is the **full README in clean Markdown**, formatted perfectly for GitHub.
Just copyâ€“paste into your repo as `README.md`.

---

# ğŸ“Š Marketing Attribution Data Pipeline

### **Snowflake | dbt Cloud | Streamlit | Staging â†’ Transform â†’ Orchestrate â†’ Visualize**

This project demonstrates a **complete modern data pipeline** using Snowflake, dbt Cloud, and Streamlit â€” starting from ingesting raw CSVs into Snowflake, transforming them with dbt, orchestrating automated runs, and visualizing business insights in an interactive dashboard.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources/                # RAW source definitions
â”‚   â”œâ”€â”€ staging/                # Cleaned and standardized models
â”‚   â”œâ”€â”€ intermediate/           # Aggregations / rollups
â”‚   â””â”€â”€ marts/                  # Final analytics tables
â”‚
â”œâ”€â”€ snapshots/                  # dbt snapshots (if enabled)
â”œâ”€â”€ tests/                      # Custom and generic dbt tests
â”œâ”€â”€ streamlit_app/              # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ packages.yml                # dbt packages (dbt_utils)
â”œâ”€â”€ dbt_project.yml             # dbt configuration
â””â”€â”€ README.md                   # You're reading this!
```

---

# ğŸš€ Project Overview

This project simulates a **6-month marketing attribution dataset** with channels such as Google Ads, Meta Ads, and web session + order data.
The pipeline covers:

### **1ï¸âƒ£ Loading data â†’ Snowflake RAW**

* Upload CSV files from local machine using a **Snowflake stage**
* Create RAW tables
* Ingest data using `COPY INTO`

### **2ï¸âƒ£ Transforming with dbt Cloud**

* Create staging models (`stg_google_ads`, `sessions_clean`, `orders_clean`)
* Build intermediate models (daily spend, attribution inputs)
* Build final marts (channel performance, ROAS, conversions)
* Add dbt tests, snapshots, documentation, and lineage

### **3ï¸âƒ£ Orchestrating with dbt Cloud Jobs**

* Schedule daily `dbt build`
* Run `dbt test` automatically
* (Optional) Run snapshots + exposures

### **4ï¸âƒ£ Visualizing in Streamlit**

* Build an interactive dashboard to view:

  * Daily spend trends
  * Clicks, impressions, orders, revenue
  * CAC, ROAS, CPV metrics
  * Campaign filtering + comparison
  * Budget simulation (increase spend â†’ project impact)

---

# ğŸ§Š 1. Loading Data Into Snowflake

### **Create and use database + schemas**

```sql
CREATE DATABASE MARKETING_ANALYTICS;
CREATE SCHEMA RAW;
CREATE SCHEMA ANALYTICS;
```

### **Create stage**

```sql
CREATE OR REPLACE STAGE raw_stage;
```

### **Upload CSVs**

Using UI â†’ Load Files â†’ Choose `raw_stage`

Example files:

```
google_ads_6months.csv
meta_ads_6months.csv
sessions_6months.csv
orders_6months.csv
```

### **Create RAW tables**

```sql
CREATE OR REPLACE TABLE RAW.GOOGLE_ADS (...);
CREATE OR REPLACE TABLE RAW.META_ADS (...);
CREATE OR REPLACE TABLE RAW.SESSIONS (...);
CREATE OR REPLACE TABLE RAW.ORDERS (...);
```

### **Load data**

```sql
COPY INTO RAW.GOOGLE_ADS
FROM @raw_stage/google_ads_6months.csv
FILE_FORMAT = (TYPE=CSV SKIP_HEADER=1);
```

Repeat for other tables.

---

# ğŸ›  2. Building Transformations in dbt Cloud

### **Source definitions**

`models/sources/src_marketing.yml` declares RAW tables.

### **Staging models (clean data)**

Examples:

* `stg_google_ads.sql`
* `sessions_clean.sql`
* `orders_clean.sql`

### **Intermediate models**

* Daily spend aggregations
* Combine sessions + orders
* Channel performance by date

### **Analytics marts**

* `fct_marketing_performance`
* ROAS metrics, conversions, CAC

### **dbt tests**

Examples included:

* `not_null`
* `unique`
* `accepted_values`
* `relationships`
* `dbt_utils.greater_than`
* custom generic tests

### **Snapshots**

Track how spend or campaign metrics change over time.

---

# ğŸ“… 3. Orchestration With dbt Cloud Jobs

This project uses dbt Cloud to orchestrate:

### **Daily job**

```
dbt build
```

Includes:

âœ” Models
âœ” Tests
âœ” Snapshots
âœ” Documentation refresh

### Optional add-ons:

* Slack alerts
* Environment promotion
* GitHub PR triggers

---

# ğŸ“ˆ 4. Analytics Dashboard â€” Streamlit

The `streamlit_app/app.py` connects directly to Snowflake using Snowpark.

### Dashboard features:

* Channel spend vs revenue
* ROAS over time
* Campaign-level performance
* Search filters + dropdowns
* KPI cards
* Time-series line charts
* Budget scaling simulator (+10%, +20%, custom)

### Run locally:

```bash
pip install streamlit snowflake-connector-python pandas
streamlit run streamlit_app/app.py
```

---

# ğŸ§ª Data Quality With dbt Tests

### Included Tests

* **not_null** (IDs, dates, user IDs)
* **unique** (session_id, order_id)
* **greater_than / greater_than_or_equal_to** (cost > 0)
* **accepted_values** (date ranges)
* **relationships** (orders â†’ sessions)
* **custom generic tests**

  * no_future_dates
  * valid_channel
  * positive_rate

All tests passed successfully via dbt Cloud.

---

# ğŸŒ Modern Data Stack Architecture

```
Local CSV â†’ Snowflake Stage â†’ RAW Tables
â†’ dbt Staging â†’ dbt Intermediate â†’ dbt Marts
â†’ Streamlit Dashboard Visualization
â†’ dbt Cloud Jobs (Orchestration)
```

This architecture is similar to how real data teams build production ELT pipelines.

---

# ğŸ¯ Business Use Case

This pipeline answers:

* Which channels drive the highest ROAS?
* Which campaigns are overspending or underspending?
* How many sessions convert into orders?
* What happens if we increase a channel's budget?
* Which channel contributes to the highest revenue per dollar?

Useful for growth teams, analysts, and marketing leadership.

---

# ğŸš€ Future Enhancements

* Multi-touch attribution (Shapley, Markov chains)
* Incremental ingestion & SCD Type 2
* Airflow orchestration (Astro)
* ML-driven spend forecasting
* Deploy Streamlit to cloud (GCP/AWS/Streamlit Cloud)
* Add CI/CD for dbt (PR builds)

---

# ğŸ‘©â€ğŸ’» Author

**Vandana Bhumireddygari**
Masters Student â€” University of Texas at Dallas
Data Engineer & Analytics Engineer
Snowflake | dbt | Databricks | Airflow | Python






### Resources:
- Learn more about dbt [in the docs](https://docs.getdbt.com/docs/introduction)
- Check out [Discourse](https://discourse.getdbt.com/) for commonly asked questions and answers
- Join the [dbt community](https://getdbt.com/community) to learn from other analytics engineers
- Find [dbt events](https://events.getdbt.com) near you
- Check out [the blog](https://blog.getdbt.com/) for the latest news on dbt's development and best practices
